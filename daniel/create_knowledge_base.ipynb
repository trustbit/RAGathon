{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-29T14:05:58.627354Z",
     "start_time": "2024-08-29T14:05:58.520048Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T14:05:58.849766Z",
     "start_time": "2024-08-29T14:05:58.725456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import PyPDF2\n",
    "\n",
    "def text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f, strict=False)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text"
   ],
   "id": "73aa1ff879683fbb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T14:05:59.716125Z",
     "start_time": "2024-08-29T14:05:59.709001Z"
    }
   },
   "cell_type": "code",
   "source": "OPENAI_MODEL=\"gpt-4o-2024-08-06\"",
   "id": "da8cea1c74a874e0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T14:06:01.499859Z",
     "start_time": "2024-08-29T14:05:59.769678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(OPENAI_MODEL)\n",
    "def prompt_info(messages):\n",
    "    content = \" \".join([m[\"content\"] for m in messages])\n",
    "    tokens = len(enc.encode(content))\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    cost_in_eur = (tokens / 1000000)*5 \n",
    "    print(f\"Cost: {cost_in_eur}€\")"
   ],
   "id": "a7a2ffc328682dd5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T14:06:01.524690Z",
     "start_time": "2024-08-29T14:06:01.515971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FIRST ATTEMPT\n",
    "\n",
    "# system_prompt = (\"You are an assistant with the task of extracting precise information from long documents. \"\n",
    "#                  \"You will be prompted with a USER QUESTION and a DOCUMENT. Your task is to answer the USER QUESTION \"\n",
    "#                  \"precisely and concisely, using only information from the DOCUMENT. If the document does not contain \"\n",
    "#                  \"enough information to answer the question, answer only with the text 'N/A'.\")\n",
    "# \n",
    "# \n",
    "# question = \"What was the total revenue in 2022?\"\n",
    "# \n",
    "# prompt = (\"USER QUESTION\\n\\n\"\n",
    "#           f\"{question}\\n\\n\"\n",
    "#           \"DOCUMENT\\n\\n\"\n",
    "#           f\"{text}\")\n",
    "# \n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "# \n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt},\n",
    "#     {\"role\": \"user\", \"content\": prompt},\n",
    "#   ]\n",
    "# \n",
    "# prompt_info(messages)"
   ],
   "id": "e4d92c8b9fa4ec84",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T14:06:01.882061Z",
     "start_time": "2024-08-29T14:06:01.561112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "\n",
    "class Metric(str, Enum):\n",
    "    rad_expenses = \"research and development expenses\"\n",
    "    risk_management_spending = \"risk management spending\"\n",
    "    debt_to_equity_ratio = \"Debt-To-Equity ratio\"\n",
    "    number_of_stores = \"Number of stores\"\n",
    "    return_on_assets = \"Return on Assets (ROA)\"\n",
    "    return_on_equity = \"Return on Assets (ROE)\"\n",
    "    customer_acquisition_spending = \"customer acquisition spending\"\n",
    "    operating_margin = \"operating margin\"\n",
    "    market_capitalization = \"market capitalization\"\n",
    "    sustainability_initiatives_spending = \"sustainability initiatives spending\"\n",
    "    gross_profit_margin = \"Gross Profit Margin\"\n",
    "\n",
    "class CompanyRole(str, Enum):\n",
    "    ceo = \"Chief Executive Officer (CEO)\"\n",
    "    cfo = \"Chief Financial Officer (CFO)\"\n",
    "    coo = \"Chief Operating Officer (COO)\"\n",
    "    clo = \"Chief Legal Officer (CLO)\"\n",
    "    board_chairman = \"Board Chairman\"\n",
    "\n",
    "class Currency(str, Enum):\n",
    "    euro = \"EUR\"\n",
    "    us_dollar = \"USD\"\n",
    "    pound_sterling = \"GBP\"\n",
    "    other = \"OTHER\"\n",
    "\n",
    "class DocumentDataPoint(BaseModel):\n",
    "    metric_type: Metric\n",
    "    value: float\n",
    "    currency: Optional[Currency]\n",
    "    point_in_time_as_iso_date: str\n",
    "\n",
    "class CompanyRoleAssignment(BaseModel):\n",
    "    role_type: CompanyRole\n",
    "    person_name: str\n",
    "    role_assignment_started_as_iso_date: Optional[str]\n",
    "    role_assignment_ended_as_iso_date: Optional[str]\n",
    "\n",
    "class DocumentContent(BaseModel):\n",
    "    data_points: list[DocumentDataPoint]\n",
    "    company_role_assignments: list[CompanyRoleAssignment]"
   ],
   "id": "3754c275b4be0d9b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T14:06:01.912662Z",
     "start_time": "2024-08-29T14:06:01.901881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_document_content(text):\n",
    "    system_prompt = (\"You are an assistant with the task of extracting precise information from long documents. \"\n",
    "                     \"You will be prompted with the contents of a document. Your task is to extract various metrics \"\n",
    "                     \"as well as company role assignments from this document. With each metric, supply the point in \"\n",
    "                     \"time when the metric was measured according to the document,\"\n",
    "                     \"as well as the currency (if applicable). \"\n",
    "                     \"With each role assignment, supply when the role assignment started and ended, if possible\")                     \n",
    "    \n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "      ]\n",
    "    \n",
    "    prompt_info(messages)\n",
    "    response = client.beta.chat.completions.parse(\n",
    "      model=OPENAI_MODEL,\n",
    "      messages=messages,\n",
    "      response_format=DocumentContent\n",
    "    )\n",
    "    \n",
    "    formatted = {\n",
    "        \"data_points\": [\n",
    "            {\n",
    "                \"metric_type\": x.metric_type.value,\n",
    "                \"value\": x.value,\n",
    "                \"currency\": x.currency.value if x.currency else None,\n",
    "                \"point_in_time\": x.point_in_time_as_iso_date\n",
    "            }\n",
    "            for x in response.choices[0].message.parsed.data_points\n",
    "        ]\n",
    "    }\n",
    "    return formatted"
   ],
   "id": "6a634b01c9d7840a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T14:12:23.428079Z",
     "start_time": "2024-08-29T14:06:01.977558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Define the paths\n",
    "samples_dir = 'samples'\n",
    "output_dir = 'output'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read the CSV file\n",
    "with open('dataset.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        name = row['sha1'].strip().replace(',', '').replace('\"', '')  # Clean up the name to be used in filenames\n",
    "        pdf_path = os.path.join(samples_dir, f'{name}.pdf')\n",
    "        \n",
    "        # Check if the PDF file exists\n",
    "        if os.path.exists(pdf_path):\n",
    "            # Define the output path for the JSON file\n",
    "            output_path = os.path.join(output_dir, f'{name}.json')\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                print(f'{output_path} already exists; skipping.')\n",
    "            else:\n",
    "                print(f'Processing {pdf_path}...')\n",
    "    \n",
    "                try:    \n",
    "                    # Extract text from the PDF\n",
    "                    pdf_text = text_from_pdf(pdf_path)\n",
    "        \n",
    "                    # Extract structured content from the text\n",
    "                    structured_data = extract_document_content(pdf_text)\n",
    "                    \n",
    "                    structured_data[\"company_name\"] = row['name']\n",
    "        \n",
    "        \n",
    "                    # Save the structured data as JSON\n",
    "                    with open(output_path, 'w') as json_file:\n",
    "                        json.dump(structured_data, json_file, indent=4)\n",
    "        \n",
    "                    print(f'Saved structured data to {output_path}.')\n",
    "                except:\n",
    "                    print(\"Exception caught; skipping...\")\n",
    "                # break\n",
    "        else:\n",
    "            # print(f'File not found: {pdf_path}')\n",
    "            pass"
   ],
   "id": "36c534d070c07b64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/d81bbc64a4160b9946fea7a895f80e6201f52f27.json already exists; skipping.\n",
      "output/608c5097dfc6e83505fd2259ad862dcec11a3f96.json already exists; skipping.\n",
      "output/3696c1b29566acc1eafc704ee5737fb3ae6f3d1d.json already exists; skipping.\n",
      "output/99be213e4e689294ebae809bfa6a1b5024076286.json already exists; skipping.\n",
      "output/71b04e0248ecf758990a0ab77bd69344be63bcf4.json already exists; skipping.\n",
      "Processing samples/6b79f1c1de9d0e39a4576dcd4585849b9465b402.pdf...\n",
      "Tokens: 219727\n",
      "Cost: 1.098635€\n",
      "Exception caught; skipping...\n",
      "Processing samples/40b5cfe0d7bbf59e186492bfbe1b5002d44af332.pdf...\n",
      "Tokens: 77178\n",
      "Cost: 0.38588999999999996€\n",
      "Saved structured data to output/40b5cfe0d7bbf59e186492bfbe1b5002d44af332.json.\n",
      "Processing samples/faf8d7d79152d61279eda1cfb58b8236ce2f82fa.pdf...\n",
      "Tokens: 27773\n",
      "Cost: 0.138865€\n",
      "Saved structured data to output/faf8d7d79152d61279eda1cfb58b8236ce2f82fa.json.\n",
      "Processing samples/4b525836a5d7cb75489f6d93a3b1cf2b8f039bf2.pdf...\n",
      "Tokens: 136290\n",
      "Cost: 0.68145€\n",
      "Exception caught; skipping...\n",
      "Processing samples/ea0757d27fa67cd347d9f046b939a911f5c9a08d.pdf...\n",
      "Tokens: 19115\n",
      "Cost: 0.095575€\n",
      "Saved structured data to output/ea0757d27fa67cd347d9f046b939a911f5c9a08d.json.\n",
      "Processing samples/9e703e719d94af786af5511c823ff86e9f04c070.pdf...\n",
      "Exception caught; skipping...\n",
      "Processing samples/dfb1e552b18e116105d9125d9becafa443950e97.pdf...\n",
      "Tokens: 55522\n",
      "Cost: 0.27761€\n",
      "Saved structured data to output/dfb1e552b18e116105d9125d9becafa443950e97.json.\n",
      "Processing samples/e51b7204b91cbe7709bd3218e7d2d0c2b8dbb438.pdf...\n",
      "Tokens: 53463\n",
      "Cost: 0.26731499999999997€\n",
      "Saved structured data to output/e51b7204b91cbe7709bd3218e7d2d0c2b8dbb438.json.\n",
      "Processing samples/bd5041c3e6909d92a7a88e4fb10dd8651df33228.pdf...\n",
      "Tokens: 143670\n",
      "Cost: 0.7183499999999999€\n",
      "Exception caught; skipping...\n",
      "Processing samples/58a5f9f5c83159e63602b0b1dd27c27fb945c0e9.pdf...\n",
      "Tokens: 127457\n",
      "Cost: 0.6372849999999999€\n",
      "Exception caught; skipping...\n",
      "Processing samples/9ff4e041732c9841d5423e6ea0bbd6a0320df9ff.pdf...\n",
      "Tokens: 44811\n",
      "Cost: 0.22405499999999998€\n",
      "Saved structured data to output/9ff4e041732c9841d5423e6ea0bbd6a0320df9ff.json.\n",
      "Processing samples/dd78f748262b8ffa62de6484143ff55b38af24c7.pdf...\n",
      "Tokens: 199201\n",
      "Cost: 0.9960049999999999€\n",
      "Exception caught; skipping...\n",
      "Processing samples/d734bac4a4815e616d84083ad4d3844655321215.pdf...\n",
      "Tokens: 140962\n",
      "Cost: 0.70481€\n",
      "Exception caught; skipping...\n",
      "Processing samples/9ae3bb21564a5098c4b4d6450655c22eff85deae.pdf...\n",
      "Exception caught; skipping...\n",
      "Processing samples/053b7cb83115789346e2a9efc7e2e640851653ff.pdf...\n",
      "Tokens: 79655\n",
      "Cost: 0.39827500000000005€\n",
      "Saved structured data to output/053b7cb83115789346e2a9efc7e2e640851653ff.json.\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
