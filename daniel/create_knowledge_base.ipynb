{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-30T07:55:07.543590Z",
     "start_time": "2024-08-30T07:55:07.533320Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T07:55:07.669401Z",
     "start_time": "2024-08-30T07:55:07.638870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import PyPDF2\n",
    "\n",
    "def text_from_pdf(file_path):\n",
    "    # Initialize an empty list to store the text of each page\n",
    "    pages_text = []\n",
    "\n",
    "    # Read the PDF file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f, strict=False)\n",
    "        for page in reader.pages:\n",
    "            pages_text.append(page.extract_text())\n",
    "    \n",
    "    # Combine all the text to check the total token count\n",
    "    full_text = \"\\n\".join(pages_text)\n",
    "    \n",
    "    # Initialize tiktoken encoding for the specified model\n",
    "    encoding = tiktoken.encoding_for_model(OPENAI_MODEL)\n",
    "    \n",
    "    # Count the number of tokens in the extracted text\n",
    "    num_tokens = len(encoding.encode(full_text))\n",
    "    \n",
    "    # Check if the number of tokens exceeds the limit (128K)\n",
    "    if num_tokens > 128000:\n",
    "        # Split the pages into two roughly equal parts\n",
    "        mid_point = len(pages_text) // 2\n",
    "        part1 = \"\\n\".join(pages_text[:mid_point])\n",
    "        part2 = \"\\n\".join(pages_text[mid_point:])\n",
    "        return [part1, part2]\n",
    "    else:\n",
    "        # Return the full text as a single-element list\n",
    "        return [full_text]"
   ],
   "id": "73aa1ff879683fbb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T07:55:07.781020Z",
     "start_time": "2024-08-30T07:55:07.778892Z"
    }
   },
   "cell_type": "code",
   "source": "OPENAI_MODEL=\"gpt-4o-2024-08-06\"",
   "id": "da8cea1c74a874e0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T07:55:08.283501Z",
     "start_time": "2024-08-30T07:55:07.826726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(OPENAI_MODEL)\n",
    "def prompt_info(messages):\n",
    "    content = \" \".join([m[\"content\"] for m in messages])\n",
    "    tokens = len(enc.encode(content))\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    cost_in_eur = (tokens / 1000000)*5 \n",
    "    print(f\"Cost: {cost_in_eur}€\")"
   ],
   "id": "a7a2ffc328682dd5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T07:55:08.292168Z",
     "start_time": "2024-08-30T07:55:08.290007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FIRST ATTEMPT\n",
    "\n",
    "# system_prompt = (\"You are an assistant with the task of extracting precise information from long documents. \"\n",
    "#                  \"You will be prompted with a USER QUESTION and a DOCUMENT. Your task is to answer the USER QUESTION \"\n",
    "#                  \"precisely and concisely, using only information from the DOCUMENT. If the document does not contain \"\n",
    "#                  \"enough information to answer the question, answer only with the text 'N/A'.\")\n",
    "# \n",
    "# \n",
    "# question = \"What was the total revenue in 2022?\"\n",
    "# \n",
    "# prompt = (\"USER QUESTION\\n\\n\"\n",
    "#           f\"{question}\\n\\n\"\n",
    "#           \"DOCUMENT\\n\\n\"\n",
    "#           f\"{text}\")\n",
    "# \n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "# \n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt},\n",
    "#     {\"role\": \"user\", \"content\": prompt},\n",
    "#   ]\n",
    "# \n",
    "# prompt_info(messages)"
   ],
   "id": "e4d92c8b9fa4ec84",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T07:55:08.422953Z",
     "start_time": "2024-08-30T07:55:08.334584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "\n",
    "class Metric(str, Enum):\n",
    "    rad_expenses = \"research and development expenses\"\n",
    "    risk_management_spending = \"risk management spending\"\n",
    "    debt_to_equity_ratio = \"Debt-To-Equity ratio\"\n",
    "    number_of_stores = \"Number of stores\"\n",
    "    return_on_assets = \"Return on Assets (ROA)\"\n",
    "    return_on_equity = \"Return on Assets (ROE)\"\n",
    "    customer_acquisition_spending = \"customer acquisition spending\"\n",
    "    operating_margin = \"operating margin\"\n",
    "    market_capitalization = \"market capitalization\"\n",
    "    sustainability_initiatives_spending = \"sustainability initiatives spending\"\n",
    "    gross_profit_margin = \"Gross Profit Margin\"\n",
    "\n",
    "class CompanyRole(str, Enum):\n",
    "    ceo = \"Chief Executive Officer (CEO)\"\n",
    "    cfo = \"Chief Financial Officer (CFO)\"\n",
    "    coo = \"Chief Operating Officer (COO)\"\n",
    "    clo = \"Chief Legal Officer (CLO)\"\n",
    "    board_chairman = \"Board Chairman\"\n",
    "\n",
    "class Currency(str, Enum):\n",
    "    euro = \"EUR\"\n",
    "    us_dollar = \"USD\"\n",
    "    great_britain_pound = \"GBP\"\n",
    "    australian_dollar = \"AUD\"\n",
    "    other = \"OTHER\"\n",
    "\n",
    "class DocumentDataPoint(BaseModel):\n",
    "    metric_type: Metric\n",
    "    value: float\n",
    "    currency: Optional[Currency]\n",
    "    point_in_time_as_iso_date: str\n",
    "\n",
    "class CompanyRoleAssignment(BaseModel):\n",
    "    role_type: CompanyRole\n",
    "    person_name: str\n",
    "    role_assignment_started_as_iso_date: Optional[str]\n",
    "    role_assignment_ended_as_iso_date: Optional[str]\n",
    "\n",
    "class DocumentContent(BaseModel):\n",
    "    data_points: list[DocumentDataPoint]\n",
    "    company_role_assignments: list[CompanyRoleAssignment]"
   ],
   "id": "3754c275b4be0d9b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T07:55:08.434727Z",
     "start_time": "2024-08-30T07:55:08.430343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_document_content(text):\n",
    "    system_prompt = (\"You are an assistant with the task of extracting precise information from long documents. \"\n",
    "                     \"You will be prompted with the contents of a document. Your task is to extract various metrics \"\n",
    "                     \"as well as company role assignments from this document. With each metric, supply the point in \"\n",
    "                     \"time when the metric was measured according to the document,\"\n",
    "                     \"as well as the currency (if applicable). \"\n",
    "                     \"With each role assignment, supply when the role assignment started and ended, if possible\")                     \n",
    "    \n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "      ]\n",
    "    \n",
    "    prompt_info(messages)\n",
    "    response = client.beta.chat.completions.parse(\n",
    "      model=OPENAI_MODEL,\n",
    "      messages=messages,\n",
    "      response_format=DocumentContent\n",
    "    )\n",
    "    \n",
    "    data_points = [\n",
    "        {\n",
    "            \"metric_type\": x.metric_type.value,\n",
    "            \"value\": x.value,\n",
    "            \"currency\": x.currency.value if x.currency else None,\n",
    "            \"point_in_time\": x.point_in_time_as_iso_date\n",
    "        }\n",
    "        for x in response.choices[0].message.parsed.data_points\n",
    "    ]\n",
    "    \n",
    "    role_assignments = [\n",
    "        {\n",
    "            \"role_type\": x.role_type.value,\n",
    "            \"person_name\": x.person_name,\n",
    "            \"role_assignment_started_as_iso_date\": x.role_assignment_started_as_iso_date,\n",
    "            \"role_assignment_ended_as_iso_date\": x.role_assignment_ended_as_iso_date\n",
    "        }\n",
    "        for x in response.choices[0].message.parsed.company_role_assignments\n",
    "    ]\n",
    "    \n",
    "    formatted = {\n",
    "        \"data_points\": data_points,\n",
    "        \"role_assignments\": role_assignments\n",
    "    }\n",
    "    return formatted"
   ],
   "id": "6a634b01c9d7840a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T08:01:12.646547Z",
     "start_time": "2024-08-30T07:55:08.481692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the paths\n",
    "samples_dir = 'samples'\n",
    "output_dir = 'output'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read the CSV file\n",
    "with open('dataset.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        name = row['sha1'].strip().replace(',', '').replace('\"', '')  # Clean up the name to be used in filenames\n",
    "        pdf_path = os.path.join(samples_dir, f'{name}.pdf')\n",
    "        \n",
    "        # Check if the PDF file exists\n",
    "        if os.path.exists(pdf_path):\n",
    "            # Define the output path for the JSON file\n",
    "            output_path = os.path.join(output_dir, f'{name}.json')\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                print(f'{output_path} already exists; skipping.')\n",
    "            else:\n",
    "                print(f'Processing {pdf_path}...')\n",
    "    \n",
    "                try:    \n",
    "                    # Extract text from the PDF\n",
    "                    pdf_texts = text_from_pdf(pdf_path)\n",
    "        \n",
    "                    # Extract structured content from the text\n",
    "                    structured_datas = [extract_document_content(pdf_text) for pdf_text in pdf_texts]\n",
    "                    \n",
    "                    structured_data = {\n",
    "                        \"company_name\": row['name'],\n",
    "                        \"data_points\": [item for d in structured_datas for item in d[\"data_points\"]],\n",
    "                        \"role_assignments\": [item for d in structured_datas for item in d[\"role_assignments\"]]\n",
    "                    }\n",
    "\n",
    "                    # Save the structured data as JSON\n",
    "                    with open(output_path, 'w') as json_file:\n",
    "                        json.dump(structured_data, json_file, indent=4)\n",
    "        \n",
    "                    print(f'Saved structured data to {output_path}.')\n",
    "                except Exception as error:\n",
    "                    print(error)\n",
    "                    print(\"Exception caught; skipping...\")\n",
    "                # Uncomment this to only work with the first PDF.\n",
    "                # break\n",
    "        else:\n",
    "            # The file was not found. We ignore this, since we are only working\n",
    "            # with a small sample.\n",
    "            pass"
   ],
   "id": "36c534d070c07b64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/d81bbc64a4160b9946fea7a895f80e6201f52f27.json already exists; skipping.\n",
      "Processing samples/608c5097dfc6e83505fd2259ad862dcec11a3f96.pdf...\n",
      "Tokens: 50886\n",
      "Cost: 0.25443€\n",
      "Saved structured data to output/608c5097dfc6e83505fd2259ad862dcec11a3f96.json.\n",
      "Processing samples/3696c1b29566acc1eafc704ee5737fb3ae6f3d1d.pdf...\n",
      "Tokens: 60018\n",
      "Cost: 0.30009€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved structured data to output/3696c1b29566acc1eafc704ee5737fb3ae6f3d1d.json.\n",
      "Processing samples/99be213e4e689294ebae809bfa6a1b5024076286.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 98482\n",
      "Cost: 0.49241€\n",
      "Saved structured data to output/99be213e4e689294ebae809bfa6a1b5024076286.json.\n",
      "Processing samples/71b04e0248ecf758990a0ab77bd69344be63bcf4.pdf...\n",
      "Tokens: 99872\n",
      "Cost: 0.49936€\n",
      "Saved structured data to output/71b04e0248ecf758990a0ab77bd69344be63bcf4.json.\n",
      "Processing samples/6b79f1c1de9d0e39a4576dcd4585849b9465b402.pdf...\n",
      "Tokens: 134731\n",
      "Cost: 0.6736549999999999€\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 135162 tokens (including 416 in the response_format schemas.). Please reduce the length of the messages or schemas.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Exception caught; skipping...\n",
      "Processing samples/40b5cfe0d7bbf59e186492bfbe1b5002d44af332.pdf...\n",
      "Tokens: 77177\n",
      "Cost: 0.385885€\n",
      "Saved structured data to output/40b5cfe0d7bbf59e186492bfbe1b5002d44af332.json.\n",
      "Processing samples/faf8d7d79152d61279eda1cfb58b8236ce2f82fa.pdf...\n",
      "Tokens: 27773\n",
      "Cost: 0.138865€\n",
      "Saved structured data to output/faf8d7d79152d61279eda1cfb58b8236ce2f82fa.json.\n",
      "Processing samples/4b525836a5d7cb75489f6d93a3b1cf2b8f039bf2.pdf...\n",
      "Tokens: 71450\n",
      "Cost: 0.35725€\n",
      "Tokens: 64926\n",
      "Cost: 0.32463€\n",
      "Saved structured data to output/4b525836a5d7cb75489f6d93a3b1cf2b8f039bf2.json.\n",
      "Processing samples/ea0757d27fa67cd347d9f046b939a911f5c9a08d.pdf...\n",
      "Tokens: 19114\n",
      "Cost: 0.09556999999999999€\n",
      "Saved structured data to output/ea0757d27fa67cd347d9f046b939a911f5c9a08d.json.\n",
      "Processing samples/9e703e719d94af786af5511c823ff86e9f04c070.pdf...\n",
      "PyCryptodome is required for AES algorithm\n",
      "Exception caught; skipping...\n",
      "Processing samples/dfb1e552b18e116105d9125d9becafa443950e97.pdf...\n",
      "Tokens: 55521\n",
      "Cost: 0.277605€\n",
      "Saved structured data to output/dfb1e552b18e116105d9125d9becafa443950e97.json.\n",
      "Processing samples/e51b7204b91cbe7709bd3218e7d2d0c2b8dbb438.pdf...\n",
      "Tokens: 53462\n",
      "Cost: 0.26731€\n",
      "Saved structured data to output/e51b7204b91cbe7709bd3218e7d2d0c2b8dbb438.json.\n",
      "Processing samples/bd5041c3e6909d92a7a88e4fb10dd8651df33228.pdf...\n",
      "Tokens: 80984\n",
      "Cost: 0.40492€\n",
      "Tokens: 62771\n",
      "Cost: 0.313855€\n",
      "Saved structured data to output/bd5041c3e6909d92a7a88e4fb10dd8651df33228.json.\n",
      "Processing samples/58a5f9f5c83159e63602b0b1dd27c27fb945c0e9.pdf...\n",
      "Tokens: 127456\n",
      "Cost: 0.6372800000000001€\n",
      "Could not parse response content as the length limit was reached\n",
      "Exception caught; skipping...\n",
      "Processing samples/9ff4e041732c9841d5423e6ea0bbd6a0320df9ff.pdf...\n",
      "Tokens: 44811\n",
      "Cost: 0.22405499999999998€\n",
      "Saved structured data to output/9ff4e041732c9841d5423e6ea0bbd6a0320df9ff.json.\n",
      "Processing samples/dd78f748262b8ffa62de6484143ff55b38af24c7.pdf...\n",
      "Tokens: 107760\n",
      "Cost: 0.5388€\n",
      "Tokens: 91526\n",
      "Cost: 0.45763€\n",
      "Saved structured data to output/dd78f748262b8ffa62de6484143ff55b38af24c7.json.\n",
      "Processing samples/d734bac4a4815e616d84083ad4d3844655321215.pdf...\n",
      "Tokens: 76164\n",
      "Cost: 0.38082€\n",
      "Tokens: 64885\n",
      "Cost: 0.32442499999999996€\n",
      "Saved structured data to output/d734bac4a4815e616d84083ad4d3844655321215.json.\n",
      "Processing samples/9ae3bb21564a5098c4b4d6450655c22eff85deae.pdf...\n",
      "PyCryptodome is required for AES algorithm\n",
      "Exception caught; skipping...\n",
      "Processing samples/053b7cb83115789346e2a9efc7e2e640851653ff.pdf...\n",
      "Tokens: 79655\n",
      "Cost: 0.39827500000000005€\n",
      "Saved structured data to output/053b7cb83115789346e2a9efc7e2e640851653ff.json.\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
